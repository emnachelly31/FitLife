{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 13427303,
          "sourceType": "datasetVersion",
          "datasetId": 8522411
        }
      ],
      "dockerImageVersionId": 31154,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "ragnutritionlangchain",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "BUmZkQQ7LFRf"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "biloxx_nutritiondataset_path = kagglehub.dataset_download('biloxx/nutritiondataset')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "FM1QfwZPLFRg"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "b1RRFrVoLFRg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# FULL RAG NOTEBOOK (single cell)\n",
        "# =============================\n",
        "# 1) Install dependencies\n",
        "!pip install -q \\\n",
        "    langchain \\\n",
        "    langchain-community \\\n",
        "    faiss-cpu \\\n",
        "    sentence-transformers \\\n",
        "    transformers \\\n",
        "    pymupdf \\\n",
        "    accelerate \\\n",
        "    bitsandbytes \\\n",
        "    torch \\\n",
        "    tqdm\n",
        "\n",
        "# 2) Imports\n",
        "import os\n",
        "import fitz   # PyMuPDF\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
        "\n",
        "# 3) Auto-detect PDFs in ../input\n",
        "pdf_files = []\n",
        "for root, dirs, files in os.walk(\"../input\"):\n",
        "    for f in files:\n",
        "        if f.lower().endswith(\".pdf\"):\n",
        "            pdf_files.append(os.path.join(root, f))\n",
        "\n",
        "if not pdf_files:\n",
        "    raise FileNotFoundError(\"No PDF files found in ../input. Please attach a dataset with PDFs to the notebook.\")\n",
        "\n",
        "print(\"Found PDF files:\")\n",
        "for p in pdf_files:\n",
        "    print(\" -\", p)\n",
        "\n",
        "# 4) Extract text from all PDFs (concatenate)\n",
        "all_text = \"\"\n",
        "for pdf_path in pdf_files:\n",
        "    print(f\"\\nExtracting from: {pdf_path}\")\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "    except Exception as e:\n",
        "        print(f\"  Failed to open {pdf_path}: {e}\")\n",
        "        continue\n",
        "    for page in doc:\n",
        "        # get_text() is simple and usually good; if you need layout-aware extraction, adapt here\n",
        "        all_text += page.get_text()\n",
        "    doc.close()\n",
        "\n",
        "print(f\"\\nTotal extracted characters: {len(all_text)}\")\n",
        "if len(all_text) < 50:\n",
        "    raise ValueError(\"Extracted very little text — check that the PDFs actually contain extractable text (not scanned images).\")\n",
        "\n",
        "# 5) Split text into chunks for retrieval\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
        "texts = splitter.split_text(all_text)\n",
        "print(\"Total chunks:\", len(texts))\n",
        "\n",
        "# Optional: show a sample chunk\n",
        "print(\"\\n--- Sample chunk ---\\n\")\n",
        "print(texts[0][:500])\n",
        "print(\"\\n--- End sample ---\\n\")\n",
        "\n",
        "# 6) Create embeddings and FAISS vector store\n",
        "print(\"Creating embeddings... (this may take a few minutes)\")\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "# Build FAISS DB\n",
        "db = FAISS.from_texts(texts, embeddings)\n",
        "retriever = db.as_retriever(search_kwargs={\"k\": 4})\n",
        "print(\"Vector store ready (FAISS).\")\n",
        "\n",
        "# 7) Load local LLM (google/flan-t5-base)\n",
        "#    NOTE: This model is CPU-friendly but still may take time to download.\n",
        "model_name = \"google/flan-t5-base\"\n",
        "print(f\"Loading model {model_name} ... (may take a minute)\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, max_length=512)\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "print(\"Model loaded.\")\n",
        "\n",
        "# 8) Build RetrievalQA RAG chain\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    chain_type=\"stuff\",   # 'stuff' works for shorter contexts; change if you want other chain types\n",
        ")\n",
        "print(\"RAG pipeline ready. You can now ask questions.\")\n",
        "\n",
        "# 9) Interactive query loop (type 'exit' to stop)\n",
        "print(\"\\nType queries to ask about the PDF(s). Type 'exit' to quit.\")\n",
        "while True:\n",
        "    try:\n",
        "        query = input(\"\\nYour question: \").strip()\n",
        "    except Exception:\n",
        "        # In some Kaggle environments input() may not behave interactively - handle gracefully\n",
        "        print(\"Interactive input not available in this environment. To run a one-off query, set 'query' variable and call qa.run(query).\")\n",
        "        break\n",
        "    if query.lower() in (\"exit\", \"quit\"):\n",
        "        print(\"Exiting interactive loop.\")\n",
        "        break\n",
        "    if not query:\n",
        "        print(\"Please type a question or 'exit'.\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # Run retrieval + generation\n",
        "        answer = qa.run(query)\n",
        "        print(\"\\nAnswer:\\n\", answer)\n",
        "    except Exception as e:\n",
        "        print(\"Error during QA:\", e)\n",
        "        # fall back to showing top retrieved chunks (useful for debugging)\n",
        "        try:\n",
        "            docs = retriever.get_relevant_documents(query)\n",
        "            print(\"\\nTop retrieved chunks (for debugging):\")\n",
        "            for i, d in enumerate(docs[:3], 1):\n",
        "                print(f\"\\n--- chunk {i} ---\\n{d.page_content[:800]}\\n\")\n",
        "        except Exception as e2:\n",
        "            print(\"Also failed to retrieve debug docs:\", e2)\n",
        "\n",
        "# 10) Optional: save FAISS index to disk for reuse\n",
        "save_dir = \"/kaggle/working/faiss_index\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "db.save_local(save_dir)\n",
        "print(f\"FAISS index saved to: {save_dir}\")\n",
        "print(\"Done.\")\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-19T20:00:00.770441Z",
          "iopub.execute_input": "2025-10-19T20:00:00.77076Z",
          "iopub.status.idle": "2025-10-19T22:25:16.883249Z",
          "shell.execute_reply.started": "2025-10-19T20:00:00.770741Z",
          "shell.execute_reply": "2025-10-19T22:25:16.878603Z"
        },
        "id": "vFYb5_WcLFRg",
        "outputId": "34686256-beb2-43ca-8535-bd2ac252c0f2",
        "colab": {
          "referenced_widgets": [
            "f3375ab7fead40ff893795277a722f95",
            "7234f4b334b34354b13401d959ecb338",
            "615b93ab03af4b139264d96c46993410",
            "cf004cfff5c94fa9a2f42c38248dfe3f",
            "f23fc378639148dcbc4f81a283d2762f",
            "65bccacc0f4341daa3cd8e81fc9a6ef1",
            "37dcb9ccd9be4bbc9bf1762b9ef87994",
            "31c2830675c842d69d1ca92923987ee2",
            "c56998df2e524428ab62ed49b0e7035f",
            "f81ffb9ceae244318c2e5da9c6524129",
            "f826ebe07d66407d84d27c1f9c9b4d34",
            "8dca5eb3a7bf4788852a44b01eb7bf6e",
            "bb38195907014b4b867ec48e5763b8cb",
            "19cc12f8f2ec4b82bcdbf071ce0a47c5",
            "d644ea432dab402f8bb922adada7dae0",
            "6dec345553e147a7b397ab16726a2c6d",
            "47c35f3675b841bd96ffa0db6c931bd9",
            "58f080bd289f45df91913ad3cc52fe42"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m449.8/449.8 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "2025-10-19 20:01:59.712401: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760904119.990138      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760904120.068247      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Found PDF files:\n - ../input/nutritiondataset/nutrition.pdf\n\nExtracting from: ../input/nutritiondataset/nutrition.pdf\n\nTotal extracted characters: 3076893\nTotal chunks: 3582\n\n--- Sample chunk ---\n\nVITAMIN\nVITAMIN\nPANTOTHENIC\nLIFE STAGE\nVITAMIN A\nVITAMIN C\nVITAMIN D\nVITAMIN E\nVITAMIN K\nTHIAMIN\nRIBOFLAVIN\nNIACIN\nB6\nFOLATE\nB12\nACID\nBIOTIN\nCHOLINEg\nGROUP\n(\u0002G/DAY)a\n(MG/DAY)\n(\u0002G/DAY)b,c\n(MG/DAY)d\n(\u0002G/DAY)\n(MG/DAY)\n(MG/DAY)\n(MG/DAY)e\n(MG/DAY)\n(\u0002G/DAY)f\n(\u0002G/DAY)\n(MG/DAY)\n(\u0002G/DAY)\n(MG/DAY)\nINFANTS\n0–6 mo\n400*\n40*\n5*\n4*\n2.0*\n0.2*\n0.3*\n2*\n0.1*\n65*\n0.4*\n1.7*\n5*\n125*\n7–12 mo\n500*\n50*\n5*\n5*\n2.5*\n0.3*\n0.4*\n4*\n0.3*\n80*\n0.5*\n1.8*\n6*\n150*\nCHILDREN\n1–3 y\n300\n15\n5*\n6\n30*\n0.5\n0.5\n6\n0.5\n150\n0.9\n2*\n8*\n200*\n4–8 \n\n--- End sample ---\n\nCreating embeddings... (this may take a few minutes)\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_37/3478752661.py:63: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3375ab7fead40ff893795277a722f95"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7234f4b334b34354b13401d959ecb338"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "README.md: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "615b93ab03af4b139264d96c46993410"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf004cfff5c94fa9a2f42c38248dfe3f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f23fc378639148dcbc4f81a283d2762f"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65bccacc0f4341daa3cd8e81fc9a6ef1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "37dcb9ccd9be4bbc9bf1762b9ef87994"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.txt: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "31c2830675c842d69d1ca92923987ee2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c56998df2e524428ab62ed49b0e7035f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f81ffb9ceae244318c2e5da9c6524129"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f826ebe07d66407d84d27c1f9c9b4d34"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Vector store ready (FAISS).\nLoading model google/flan-t5-base ... (may take a minute)\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8dca5eb3a7bf4788852a44b01eb7bf6e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb38195907014b4b867ec48e5763b8cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "19cc12f8f2ec4b82bcdbf071ce0a47c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d644ea432dab402f8bb922adada7dae0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6dec345553e147a7b397ab16726a2c6d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "47c35f3675b841bd96ffa0db6c931bd9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "58f080bd289f45df91913ad3cc52fe42"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Device set to use cpu\n/tmp/ipykernel_37/3478752661.py:76: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n  llm = HuggingFacePipeline(pipeline=pipe)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Model loaded.\nRAG pipeline ready. You can now ask questions.\n\nType queries to ask about the PDF(s). Type 'exit' to quit.\n",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "\nYour question:  How many calories are in 100g of cooked quinoa?\n"
        },
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_37/3478752661.py:105: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n  answer = qa.run(query)\nToken indices sequence length is longer than the specified maximum sequence length for this model (1301 > 512). Running this sequence through the model will result in indexing errors\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nAnswer:\n 100g of cooked quinoa.\n",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "\nYour question:  What vitamins are especially important for bone health?\n"
        },
        {
          "name": "stdout",
          "text": "\nAnswer:\n Vitamin D\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_37/3478752661.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nYour question: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# In some Kaggle environments input() may not behave interactively - handle gracefully\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ],
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "UPqlkbGSLFRh"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}